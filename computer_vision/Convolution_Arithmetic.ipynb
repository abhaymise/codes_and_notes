{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5b81b2",
   "metadata": {},
   "source": [
    "## Image representation \n",
    "\n",
    "Computers sees an input image as array of pixels and it depends on the image resolution. Based on the image resolution, it will see h x w x d( h = Height, w = Width, d = Dimension ). \n",
    "\n",
    "Eg., An image of 6 x 6 x 3 array of matrix of RGB (3 refers to RGB values) and an image of 4 x 4 x 1 array of matrix of grayscale image.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://www.matlabsolutions.com/images/matrix1.png\" alt=\"ml\" style=\"width: 100px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\">image matrix with RGB channel</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f8864",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da860f90",
   "metadata": {},
   "source": [
    "# Filters, Kernels, and Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63298682",
   "metadata": {},
   "source": [
    "## Filters and Kernels in Images\n",
    "\n",
    "**Filters** (also known as **kernels**) are small matrices used to perform convolution operations on larger matrices, typically representing images. Each filter slides over the input matrix (image) to produce an output matrix (feature map).\n",
    "\n",
    "- **Filter/Kernel Matrix**: A filter is usually a small matrix with dimensions \\( k x k \\), where \\( k \\) is typically odd (e.g., 3x3, 5x5). \n",
    "\n",
    "- used to apply effects like the ones you might find in Photoshop or Gimp, such as blurring, sharpening, outlining or embossing. \n",
    "- They're also used in machine learning for 'feature extraction', a technique for determining the most important portions of an image.\n",
    "\n",
    "The values in this matrix are the weights that are either \n",
    "\n",
    "- **static** in case of sobel,gaussian kernels\n",
    "- **learned** during the training process of a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f2095",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/360/lec/w04/imgs/sobel1.png\" alt=\"ml\" style=\"width: 500px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\">convolution kernel being used for edge detection</div>\n",
    "\n",
    "<div style=\"text-align: center;\" markdown=\"1\">\n",
    "<a href=\"https://setosa.io/ev/image-kernels/\">A beautiful visualization to play around with various static filters</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a620ac4",
   "metadata": {},
   "source": [
    "## Convolution Operation\n",
    "\n",
    "**Convolution** is the process of applying a filter to an input matrix to produce a feature map. This operation involves sliding the filter over the input matrix and computing the dot product of the filter with the overlapping part of the input matrix at each position.\n",
    "\n",
    "**Definition**: Convolution is a mathematical operation used to combine two sets of information. In signal processing, it describes the way in which a signal is modified by a system.\n",
    "\n",
    "**Signal Processing**: Convolution in signal processing can be used for filtering signals, applying various effects, and understanding the system's response to different inputs.\n",
    "\n",
    "**Image Processing**\n",
    "\n",
    "**Application**: Convolution is used to apply filters to images, which can detect edges, blur the image, sharpen it, etc.\n",
    "\n",
    "**Process**: An image is represented as a matrix of pixel values. A filter (or kernel) is a smaller matrix that is slid over the image matrix to produce a transformed image. Each position of the filter involves a dot product of the filter coefficients and the corresponding image pixel values.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c966ad",
   "metadata": {},
   "source": [
    "### Dimensions and Shapes\n",
    "\n",
    "1. **Input Matrix (Image)**:\n",
    "   - Let's consider an input image of size \\( H x W \\).\n",
    "   - If the image has \\( C \\) channels (e.g., RGB image), the dimensions become \\( H X W X C \\).\n",
    "\n",
    "2. **Filter (Kernel)**:\n",
    "   - A filter has dimensions \\( k X k \\).\n",
    "   - For multi-channel images, a filter typically has dimensions \\( k X k X C \\), matching the depth of the input.\n",
    "\n",
    "3. **Output Matrix (Feature Map)**:\n",
    "   - The dimensions of the output matrix depend on the input size, filter size, and the stride (step size) with which the filter is applied.\n",
    "   - If the input is \\( H X W \\) and the filter is \\( k X k \\) with stride \\( s \\), the output dimensions \\( H' X W' \\) are calculated as:\n",
    "     \n",
    "     $ H' =[\\frac{H - k}{s}] + 1 $\n",
    "     \n",
    "     $ W' =[\\frac{W - k}{s}] + 1 $\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c483a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Filters/Kernels**: Small matrices (e.g., 3x3) used to detect patterns in the input data.\n",
    "- **Convolution**: The process of sliding a filter over the input matrix to produce a feature map.\n",
    "- **Dimensions**: \n",
    "  - Input matrix: \\( H X W \\) (or \\( H X W X C \\) for multi-channel inputs).\n",
    "  - Filter: \\( k X k \\) (or \\( k X k X C \\) for multi-channel inputs).\n",
    "  - Output matrix: Calculated based on input size, filter size, and stride.\n",
    "\n",
    "This detailed explanation covers the fundamental concepts of filters, kernels, and convolutions, including the mathematical operations and dimensional transformations involved in CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b2432",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b479d",
   "metadata": {},
   "source": [
    "## 1 filter convolution at 1 spatial patch and generates a point of feature bank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca64ab3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e32304d7",
   "metadata": {},
   "source": [
    "``` python\n",
    "# kernel and image_patch are n x n matrices\n",
    "pixel_out = np.sum(kernel * image_patch) # gives out a scalar point\n",
    "```\n",
    "\n",
    "![a](https://miro.medium.com/v2/resize:fit:800/1*hhV1YD1DCvZZle2ptot49Q.png \"Image RGB  to feature map reduction\")\n",
    "<div style=\"text-align: center;\" markdown=\"1\">1 filter convolution at 1 spatial patch and generates a scalar point</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf730b",
   "metadata": {},
   "source": [
    "## 1 filter convolution on full image and generates 2D feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c49ea1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Convolutional_Neural_Network_with_Color_Image_Filter.gif\" alt=\"ml\" style=\"width: 500px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\">1 filter convolution slides at all spatial patch on image and generates a 2D matrix</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6ac31",
   "metadata": {},
   "source": [
    "## Nx3x3xx3 filter convolution full image and generates 4D feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbef33d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "163e1b86",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:600/1*EfXAnrwUObQmFtaxcHUlVg.png\" alt=\"ml\" style=\"width: 500px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\">N filter convolution at full spatial region</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3d381",
   "metadata": {},
   "source": [
    "## Full animated view of the convolution by multiple kernels generating feature map volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856fe99",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "<img src=\"https://animatedai.github.io/media/convolution-animation-3x3-kernel.gif\" alt=\"ml\" style=\"width: 500px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\"> \n",
    "<a href=\"https://gigazine.net/gsc_news/en/20231025-animated-ai/\">image credit</a>\n",
    "animated convolution operation by a set of filter banks\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0d397",
   "metadata": {},
   "source": [
    "## Learning  Patterns in Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc0c4a",
   "metadata": {},
   "source": [
    "### Background : \n",
    "**Fully Connected Neural Network (FCNN)**\n",
    "\n",
    "Inherently an image is 3D matrix having dimension `WxH`(spatial dimension)`xD`(channel).\n",
    "\n",
    "We can do any pixel level learning by casting this `WXHXD` into `WXHXD`X`1` vector and then passing through an approximation network like FCNN. This however will have few demerits like :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d6c26",
   "metadata": {},
   "source": [
    "1. **Architecture**:\n",
    "   - Each neuron in one layer is connected to every neuron in the next layer.\n",
    "   - Dense connections across layers.\n",
    "\n",
    "2. **Feature Learning**:\n",
    "   - No explicit mechanism for spatial hierarchies.\n",
    "   - Requires flattened input (e.g., images converted to 1D vectors).\n",
    "\n",
    "3. **Parameter Efficiency**:\n",
    "   - Large number of parameters due to dense connections.\n",
    "   - Prone to overfitting with limited training data.\n",
    "\n",
    "4. **Computation**:\n",
    "   - High computational cost due to the large number of connections.\n",
    "   - Not inherently suited for processing spatial data.\n",
    "\n",
    "5. **Translation Invariance**:\n",
    "   - Lacks inherent translation invariance.\n",
    "   - Sensitive to the position of features in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e094ebf",
   "metadata": {},
   "source": [
    "## Jargons\n",
    "\n",
    "### Kernels\n",
    "\n",
    "**Kernels** (also known as **filters**) are small matrices used to perform convolution operations on larger matrices, typically representing images. Each kernel slides over the input matrix (image) to produce an output matrix (feature map). The kernel values are the weights applied during the convolution process.\n",
    "\n",
    "### Predefined Kernels\n",
    "\n",
    "**Predefined kernels** are kernels with fixed values that are not learned from data. They are typically used in traditional image processing for tasks such as edge detection, sharpening, and blurring. Examples include the Sobel kernel for edge detection and the Gaussian kernel for blurring.\n",
    "\n",
    "### Learned Kernels/Convolution Filters\n",
    "\n",
    "**Learned kernels** (also known as **convolution filters**) are kernels whose values are learned during the training process of a convolutional neural network (CNN). These kernels are adjusted to detect specific features in the input data, such as edges, textures, and patterns, contributing to the network's ability to perform tasks like image classification and object detection.\n",
    "\n",
    "### Filters\n",
    "\n",
    "**Filters** are another term for kernels. They are the small matrices used in convolution operations to detect specific features in the input data. Filters are applied across the input matrix to produce activation maps.\n",
    "\n",
    "### Filterbank\n",
    "\n",
    "A **filterbank** refers to a collection of multiple filters (kernels) applied to the input data in a single convolutional layer. Each filter in the filterbank detects different features, resulting in multiple activation maps that are combined to form the layer's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e941976",
   "metadata": {},
   "source": [
    "### Padding and Strides\n",
    "\n",
    "- **Padding**: Padding involves adding extra pixels (usually zeros) around the border of the input image before applying the convolution operation. Padding helps control the spatial dimensions of the output feature map and can preserve the input dimensions. Common types of padding include:\n",
    "  - **Valid Padding**: No padding is added, resulting in a smaller output.\n",
    "  - **Same Padding**: Padding is added to keep the output dimensions the same as the input dimensions.\n",
    "\n",
    "- **Strides**: The stride is the step size with which the filter moves across the input image during the convolution operation. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means the filter moves two pixels at a time. Stride affects the spatial dimensions of the output feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e03ef6",
   "metadata": {},
   "source": [
    "### Receptive Field\n",
    "\n",
    "The **receptive field** is the region of the input image that a particular neuron in a CNN layer is responsive to. As the network goes deeper, neurons in higher layers have larger receptive fields, allowing them to capture more global and complex features from the input image.\n",
    "\n",
    "### Activation Map\n",
    "\n",
    "An **activation map** (or **feature map**) is the output of a convolutional layer after the convolution operation and the application of an activation function. It represents the activations (i.e., responses) of the neurons in the layer to different parts of the input image, highlighting detected features.\n",
    "\n",
    "We can compute the spatial size of the output volume as a function of the input volume size `(W)`, the receptive field size of the Conv Layer neurons `(F)`, the stride with which they are applied `(S)`, and the amount of zero padding used `(P)` on the border. \n",
    "\n",
    "$$ \\text{the spatial size of the output volume} = \\frac{(W-F+2P)}{S}+1$$\n",
    "\n",
    "> **For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27a75d",
   "metadata": {},
   "source": [
    "<table>\n",
    "     <tr>\n",
    "         <th>\n",
    "<a href=\"https://github.com/vdumoulin/conv_arithmetic?tab=readme-ov-file\">  \n",
    "   image credit </a></th>\n",
    "   \n",
    " </tr>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif\" alt=\"ml\" style=\"width: 200px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\"> P=0,S=1,W=4,F=3</div>\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides.gif\" alt=\"ml\" style=\"width: 200px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\"> P=1,S=2,W=5,F=3 </div>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/same_padding_no_strides.gif\" alt=\"ml\" style=\"width: 200px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\"> P=1,S=1,W=5,F=3 </div>\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/full_padding_no_strides.gif\" alt=\"ml\" style=\"width: 200px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\"> P=2,S=1,W=5,F=3 </div>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96c657",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\" markdown=\"1\"> \n",
    "<a href=\"https://ezyang.github.io/convolution-visualizer/\">Receptive field to Feature map reduction , visualization with padding, stride and filter sizes</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6391613",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "An **activation function** is a non-linear function applied to the output of a convolution operation (or any other neural network layer) to introduce non-linearity into the model. Common activation functions include:\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: $$\\text{ReLU}(x) = \\max(0, x) $$\n",
    "- **Sigmoid**: $$\\sigma(x) = \\frac{1}{1 + e^{-x}}  $$\n",
    "- **Tanh**: $$\\text{Tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "\n",
    "### Pooling\n",
    "\n",
    "**Pooling** (or subsampling) is a downsampling operation that reduces the spatial dimensions of the input, helping to reduce the number of parameters and computations in the network. It also helps make the network invariant to small translations of the input. Common types of pooling include:\n",
    "- **Max Pooling**: Takes the maximum value from each patch of the input.\n",
    "- **Average Pooling**: Takes the average value from each patch of the input.\n",
    "\n",
    "Pooling layers are typically used after convolutional layers to progressively reduce the spatial dimensions of the feature maps while retaining important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63acd35b",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "\n",
    "At a high level, CNN architectures contain an upstream feature extractor followed by a downstream classifier. The feature extraction segment is sometimes referred to as the “backbone” or “body” of the network. The classifier is sometimes referred to as the “head” of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79216f50",
   "metadata": {},
   "source": [
    "1. **Architecture**:\n",
    "   - Consists of convolutional layers, pooling layers, and sometimes fully connected layers.\n",
    "   - Convolutional layers apply filters across the input.\n",
    "\n",
    "2. **Feature Learning**:\n",
    "   - Captures spatial hierarchies of features.\n",
    "   - Preserves spatial relationships in the input data.\n",
    "\n",
    "3. **Parameter Efficiency**:\n",
    "   - Parameter sharing: each filter is used across the entire input.\n",
    "   - Sparse connections: each neuron is connected to a local region of the input.\n",
    "\n",
    "4. **Computation**:\n",
    "   - Lower computational cost compared to FCNNs for large inputs.\n",
    "   - Efficient for image and spatial data processing.\n",
    "\n",
    "5. **Translation Invariance**:\n",
    "   - More robust to translation of features due to convolution and pooling.\n",
    "   - Learns features regardless of their position in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a17683",
   "metadata": {},
   "source": [
    "## Trainable convolution kernel in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59171318",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-convolution-one-filter-example.png\" alt=\"ml\" style=\"width: 800px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\"> \n",
    "<a href=\"https://learnopencv.com\">image credit</a>\n",
    "one full convolution layer in in a CNN\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a2ac6",
   "metadata": {},
   "source": [
    "## Parameter Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835229e1",
   "metadata": {},
   "source": [
    "### Fully Connected Layer\n",
    "\n",
    "A fully connected layer has a weight for every connection between each input pixel and each output neuron, leading to a massive number of parameters when the input size is large.\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "A convolutional layer uses shared weights in the form of filters that are convolved over the input image. This means the same filter is applied across different regions of the input, drastically reducing the number of parameters while still being able to capture spatial features.\n",
    "\n",
    "### Example Calculation with Dimensions\n",
    "\n",
    "#### Input Image\n",
    "\n",
    "$$\n",
    "\\text{Dimensions}: 32 \\times 32 \\times 3\n",
    "$$\n",
    "\n",
    "#### Fully Connected Layer\n",
    "\n",
    "$$Input size: 32×32×3=3072$$\n",
    "$$Output size: 1000$$\n",
    "\n",
    "$$\n",
    "\\text{Weights}: 3072 \\times 1000 = 3,072,000\n",
    "$$\n",
    "$$\n",
    "\\text{Biases}: 1000\n",
    "$$\n",
    "$$\n",
    "\\text{Total Parameters}: 3,072,000 + 1000 = 3,073,000\n",
    "$$\n",
    "\n",
    "#### Convolutional Layer\n",
    "\n",
    "$$\n",
    "\\text{Filter Size}: 3 \\times 3 \\times 3 = 27\n",
    "$$\n",
    "$$\n",
    "\\text{Number of Filters}: 32\n",
    "$$\n",
    "$$\n",
    "\\text{Weights}: 27 \\times 32 = 864\n",
    "$$\n",
    "$$\n",
    "\\text{Biases}: 32\n",
    "$$\n",
    "$$\n",
    "\\text{Total Parameters}: 864 + 32 = 896\n",
    "$$\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "#### Parameter Reduction Comparison\n",
    "\n",
    "\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "\\text{Layer Type} & \\text{Total Parameters} \\\\\n",
    "\\hline\n",
    "\\text{Fully Connected Layer} & 3,073,000 \\\\\n",
    "\\hline\n",
    "\\text{Convolutional Layer} & 896 \\\\\n",
    "\\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3b397",
   "metadata": {},
   "source": [
    "\n",
    "#### Magnitude of Reduction\n",
    "\n",
    "$\n",
    "\\text{Reduction Ratio} = \\frac{\\text{FCNN parameters}}{\\text{CNN parameters}} = \\frac{3,073,000}{896} \\approx 3,429\n",
    "$\n",
    "\n",
    "So, a CNN layer with 32 filters reduces the number of parameters by a factor of approximately 3,429 compared to a fully connected layer.\n",
    "This significant reduction allows CNNs to be more efficient and scalable, particularly for large input sizes, while still being able to effectively capture and learn spatial features in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a5a0a",
   "metadata": {},
   "source": [
    "\n",
    "## Hierachical Learning\n",
    "\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-cnn-hierarchical-structure-2048x1181.png\" alt=\"ml\" style=\"width: 800px; margin-left: auto; margin-right: auto;\"/>\n",
    "<div style=\"text-align: center;\" markdown=\"1\"> \n",
    "<a href=\"https://learnopencv.com\">image credit</a>\n",
    "hierarchical learning of features\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda53e2",
   "metadata": {},
   "source": [
    "## Comparison between Fully Connected Neural Networks (FCNN) and Convolutional Neural Networks (CNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cc48c",
   "metadata": {},
   "source": [
    "## Efficiency in Capturing Features\n",
    "\n",
    "1. **Spatial Hierarchies**:\n",
    "   - CNNs inherently capture spatial hierarchies by applying multiple layers of convolutions and pooling.\n",
    "   - FCNNs lack this ability since they treat the input as a flat vector.\n",
    "\n",
    "2. **Local Patterns**:\n",
    "   - CNNs excel at detecting local patterns and combining them to form more complex features.\n",
    "   - FCNNs do not have a mechanism to naturally detect local patterns.\n",
    "\n",
    "3. **Parameter Reduction**:\n",
    "   - CNNs significantly reduce the number of parameters through parameter sharing.\n",
    "   - FCNNs have a high number of parameters due to dense connections.\n",
    "\n",
    "4. **Robustness to Input Variations**:\n",
    "   - CNNs are more robust to variations and translations in the input data.\n",
    "   - FCNNs are more sensitive to the exact position of features.\n",
    "\n",
    "## Tabular Comparison\n",
    "\n",
    "| Feature                           | Fully Connected Neural Network (FCNN) | Convolutional Neural Network (CNN) |\n",
    "|-----------------------------------|---------------------------------------|------------------------------------|\n",
    "| **Architecture**                  | Dense connections between layers      | Convolutional and pooling layers   |\n",
    "| **Feature Learning**              | No spatial hierarchies                | Captures spatial hierarchies       |\n",
    "| **Input Format**                  | Flattened vectors                     | Preserves 2D structure of images   |\n",
    "| **Parameter Efficiency**          | Large number of parameters            | Fewer parameters due to sharing and sparsity |\n",
    "| **Computation Cost**              | High                                  | Lower for large inputs             |\n",
    "| **Translation Invariance**        | Lacks translation invariance          | Inherent translation invariance    |\n",
    "| **Local Pattern Detection**       | Not specialized for local patterns    | Efficiently detects local patterns |\n",
    "| **Robustness to Input Variations**| Sensitive to position changes         | Robust to position and translation of features |\n",
    "| **Use Case Suitability**          | General-purpose tasks                 | Tasks involving spatial data (e.g., image processing) |\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, CNNs are more efficient and effective than FCNNs for tasks involving spatial data, such as image processing, due to their ability to capture spatial hierarchies, reduce the number of parameters through shared weights, and robustly detect patterns regardless of their position in the input. FCNNs, on the other hand, are more suitable for tasks where spatial structure is not as important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd19c4c",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f54c2",
   "metadata": {},
   "source": [
    "- <a href=\"https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/?fbclid=IwAR3SdRsa8Esc_VyjvjASkwQvh5VO4gr_KSxb7xALWwBWEEck59AIlee8baE\">VGG layer by layer filter bank and feature map visualization blog with code</a>\n",
    "\n",
    "- <a href=\"https://github.com/sthanhng/CNN-Visualization?tab=readme-ov-file\">keras repo</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28992fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
